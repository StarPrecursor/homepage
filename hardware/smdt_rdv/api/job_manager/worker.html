<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>smdt_rdv.job_manager.worker API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>smdt_rdv.job_manager.worker</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import concurrent.futures
import copy
import datetime
import logging
import os
import pathlib
import shutil
import time
from itertools import repeat

import numpy as np

from smdt_rdv.analyze import analyzer
from smdt_rdv.common.smdt_const import *
from smdt_rdv.data_io import db_dumper
from smdt_rdv.job_manager import config_utils, job_utils
from smdt_rdv.visualize import spectrums

logger = logging.getLogger(&#34;smdt_rdv&#34;)


class smdt_worker(object):
    &#34;&#34;&#34;Core class to execute the sMDT RDV job based on given cfg file&#34;&#34;&#34;

    def __init__(self, yaml_path: str) -&gt; None:
        &#34;&#34;&#34;Initialize executor.

        Args:
            yaml_path (str): path to yaml job config file
        &#34;&#34;&#34;
        self.job_config = None
        self.load_config(yaml_path)

    def get_config(self):
        return self.job_config

    def load_config(self, yaml_path: str) -&gt; None:
        &#34;&#34;&#34;Retrieves configurations from yaml file

        Args:
            yaml_path (str): path to yaml job config file
        &#34;&#34;&#34;
        cfg_path = job_utils.get_valid_cfg_path(yaml_path)
        yaml_dict = config_utils.load_yaml_dict(cfg_path)
        job_config_temp = config_utils.sMDT_Job_Config(yaml_dict)
        # Check whether need to import other (default) ini file first
        if hasattr(job_config_temp, &#34;config&#34;):
            if hasattr(job_config_temp.config, &#34;include&#34;):
                import_ini_path_list = job_config_temp.config.include
                if import_ini_path_list:
                    for cfg_path in import_ini_path_list:
                        self.load_config(cfg_path)
                        logger.info(f&#34;Included config: {cfg_path}&#34;)
        if self.job_config:
            self.job_config.update(yaml_dict)
        else:
            self.job_config = job_config_temp

        datestr = datetime.date.today().strftime(&#34;%Y-%m-%d&#34;)
        job_cfg = self.job_config.job
        run_cfg = self.job_config.run
        run_cfg.datestr = datestr
        run_cfg.config_collected = True

    def work(self) -&gt; None:
        &#34;&#34;&#34;Execute all planned jobs&#34;&#34;&#34;
        self.job_config.print()
        job_type = self.job_config.job.job_type
        if job_type == &#34;dump_database&#34;:
            self.work_on_database()
        elif job_type == &#34;analyze&#34;:
            self.work_on_analyze()
        else:
            logger.critical(
                f&#34;Unknown job type: {job_type}, please check &#39;job.job_type&#39;&#34;
            )
            exit(1)

    def work_on_database(self) -&gt; None:
        &#34;&#34;&#34;Dump events database according to configs&#34;&#34;&#34;
        full_config = self.job_config
        job_cfg = full_config.job
        path_cfg = full_config.path
        run_cfg = full_config.run
        perf_cfg = full_config.performance

        # initialization
        data_time = str(datetime.datetime.now())
        logger.info(f&#34;Data processed at {data_time}&#34;)

        # check output directory
        if path_cfg.database_out_dir == config_utils.UNDEFINED_CFG_STR:
            logger.error(
                &#34;Undefined output database directory, please check &#39;path.database_out_dir&#39;&#34;
            )
            exit(1)
        db_dir = pathlib.Path(path_cfg.database_out_dir)
        if db_dir.exists():
            logger.info(f&#34;Output database directory: {db_dir} already exists&#34;)
            if path_cfg.overwrite:
                logger.info(f&#34;Remove existing directory as configured&#34;)
                shutil.rmtree(db_dir)
            else:
                logger.critical(
                    f&#34;Please change database_out_dir or set path.overwrite to true&#34;
                )
                exit(1)

        # loop over files
        if path_cfg.data_in_dir == config_utils.UNDEFINED_CFG_STR:
            logger.error(
                &#34;Undefined input data file directory, please check &#39;path.data_in_dir&#39;&#34;
            )
            exit(1)
        data_dir = pathlib.Path(path_cfg.data_in_dir)
        data_list = list(data_dir.rglob(&#34;*.data&#34;))
        data_list += list(data_dir.rglob(&#34;*.data.gz&#34;))
        num_data_files = len(data_list)
        if num_data_files &gt; 0:
            logger.info(f&#34;Number of data file found: {num_data_files}&#34;)
        else:
            logger.warn(&#34;No data file found!&#34;)
        run_cfg.out_dir = pathlib.Path(path_cfg.database_out_dir)
        run_cfg.out_dir.mkdir(exist_ok=True, parents=True)
        max_files = job_cfg.max_files
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=perf_cfg.max_workers
        ) as process_executor:
            for file_id, data_path in enumerate(data_list):
                # prepare config, because only object can be pickled can work with multi-processing
                config_dict = copy.deepcopy(full_config.get_config_dict())
                # dump database
                if max_files &gt; 0 and file_id &gt;= max_files:
                    break
                logger.info(f&#34;Arranging to process data file: {data_path}&#34;)
                if perf_cfg.multi_process:
                    process_executor.submit(
                        db_dumper.dump_database_single_file, data_path, config_dict
                    )
                else:
                    db_dumper.dump_database_single_file(data_path, config_dict)

        # TODO: need to check output (shouldn&#39;t end with .tmp.db if success)
        #       if fail, try to decrease cache_size (for example 1/2) and try again
        #       as has been observed, too large cache_size may cause job to fail

    def work_on_analyze(self) -&gt; None:
        &#34;&#34;&#34;Dump events database according to configs&#34;&#34;&#34;
        full_config = self.job_config
        job_cfg = full_config.job
        path_cfg = full_config.path
        run_cfg = full_config.run
        perf_cfg = full_config.performance

        # initialization
        data_time = str(datetime.datetime.now())
        logger.info(f&#34;Data processed at {data_time}&#34;)

        ## check output directory
        result_dir = pathlib.Path(path_cfg.results_out_dir)
        if result_dir.exists():
            logger.info(f&#34;Result directory: {result_dir} already exists&#34;)
            if path_cfg.overwrite:
                logger.info(f&#34;Remove existing directory as configured&#34;)
                shutil.rmtree(result_dir)
            else:
                logger.critical(
                    f&#34;Please change results_out_dir or set path.overwrite to true&#34;
                )
                exit(1)

        ## check input database directory
        input_db_dirs = []
        if path_cfg.database_in_dir == config_utils.UNDEFINED_CFG_STR:
            logger.error(
                &#34;Undefined input database directory, please check &#39;path.database_in_dir&#39;&#34;
            )
            exit(1)
        db_in = path_cfg.database_in_dir
        if type(db_in) is list:
            for db_ele in db_in:
                input_db_dirs.append(pathlib.Path(db_ele))
        elif type(db_in) is str:
            input_db_dirs.append(pathlib.Path(db_in))
        else:
            logger.critical(
                f&#34;Unsupported database_in_dir type {type(db_in)}, please check &#39;path.data_in_dir&#39;&#34;
            )
            exit(1)
        data_list = []
        for db_dir in input_db_dirs:
            data_list += list(db_dir.rglob(&#34;*data.db&#34;))

        num_data_files = len(data_list)
        if num_data_files &gt; 0:
            logger.info(
                f&#34;Number of data file found: {num_data_files} in {len(input_db_dirs)} database directories&#34;
            )
        else:
            logger.warn(&#34;No data file found!&#34;)

        # loop over files
        max_files = job_cfg.max_files
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=perf_cfg.max_workers
        ) as process_executor:
            # prepare
            db_paths = []
            file_ids = []
            config_dict = copy.deepcopy(full_config.get_config_dict())
            # loop files
            results = []
            for file_id, db_path in enumerate(data_list):
                if max_files &gt; 0 and file_id &gt;= max_files:
                    break
                if perf_cfg.multi_process:
                    # add to multi-process input if multi-process enabled
                    db_paths.append(db_path)
                    file_ids.append(file_id)
                else:
                    # otherwise run on current input directly
                    single_result = analyzer.analyze_single_file(
                        db_path, file_id, config_dict
                    )
                    results.append(single_result)
            # run multi-process
            if perf_cfg.multi_process:
                logger.info(&#34;Analyzing with multi-processing enabled&#34;)

                # get chunk size and input chunks
                chunk_size = perf_cfg.ana_chunk_size
                if chunk_size is None or chunk_size &lt;= 0:
                    chunk_size = os.cpu_count()
                db_paths_chunks = [
                    db_paths[x : x + chunk_size]
                    for x in range(0, len(db_paths), chunk_size)
                ]
                file_ids_chunks = [
                    file_ids[x : x + chunk_size]
                    for x in range(0, len(file_ids), chunk_size)
                ]

                # get max results size
                max_results_size = perf_cfg.ana_max_results_cache
                if max_results_size is None or max_results_size &lt;= 0:
                    max_results_size = chunk_size

                # multi-process run
                for db_paths_chunk, file_ids_chunk in zip(
                    db_paths_chunks, file_ids_chunks
                ):
                    temp_results = process_executor.map(
                        analyzer.analyze_single_file,
                        db_paths_chunk,
                        file_ids_chunk,
                        repeat(config_dict),
                    )
                    results += temp_results
                    # reduce results size to save memory
                    if len(results) &gt;= max_results_size:
                        merged_result = {}
                        smdt_spectrums_lists = {
                            chamber_name: [] for chamber_name in job_cfg.chamber_names
                        }
                        for result in results:
                            for chamber_name in job_cfg.chamber_names:
                                smdt_spectrums_lists[chamber_name].append(
                                    result[chamber_name]
                                )

                        # process data
                        for chamber_name in job_cfg.chamber_names:
                            chamber_spectrums = spectrums.merge_spectrums_list(
                                smdt_spectrums_lists[chamber_name]
                            )
                            merged_result[chamber_name] = chamber_spectrums
                        results = [merged_result]

        # collect data from multiprocess outputs
        smdt_spectrums_lists = {
            chamber_name: [] for chamber_name in job_cfg.chamber_names
        }
        for result in results:
            for chamber_name in job_cfg.chamber_names:
                smdt_spectrums_lists[chamber_name].append(result[chamber_name])

        # process data
        for chamber_name in job_cfg.chamber_names:
            # merge spectrums
            logger.info(f&#34;Processing chamber {chamber_name}&#34;)
            logger.info(&#34;Merging spectrums...&#34;)
            chamber_spectrums = spectrums.merge_spectrums_list(
                smdt_spectrums_lists[chamber_name]
            )

            # check hits
            for mezz in range(MAXNUMBERMEZZANINE):
                for channel in range(MAXNUMBERCHANNEL):
                    num_hits = np.sum(chamber_spectrums[mezz][channel][&#34;tdc_lead&#34;])
                    logger.debug(f&#34;mezz {mezz} ch {channel} hits:{num_hits}&#34;)

            # plot
            range_tdc = None
            plot_range_tdc = job_cfg.spectrums_cfg.plot_range_tdc
            if plot_range_tdc:
                if (plot_range_tdc.min is not None) and (
                    plot_range_tdc.max is not None
                ):
                    range_tdc = (plot_range_tdc.min, plot_range_tdc.max)
            range_adc = None
            plot_range_adc = job_cfg.spectrums_cfg.plot_range_adc
            if plot_range_adc:
                if (plot_range_adc.min is not None) and (
                    plot_range_adc.max is not None
                ):
                    range_adc = (plot_range_adc.min, plot_range_adc.max)
            if perf_cfg.multi_process:
                max_workers = perf_cfg.max_workers
            else:
                max_workers = 1
            if job_cfg.book_spectrums:
                spectrums.plot_spectrums(
                    chamber_spectrums,
                    chamber_name=chamber_name,
                    save_dir=pathlib.Path(path_cfg.results_out_dir).joinpath(
                        f&#34;{chamber_name}/spectrums&#34;
                    ),
                    unit=job_cfg.spectrums_cfg.unit,
                    plot_bins_tdc=job_cfg.spectrums_cfg.plot_bins_tdc,
                    plot_range_tdc=range_tdc,
                    plot_bins_adc=job_cfg.spectrums_cfg.plot_bins_adc,
                    plot_range_adc=range_adc,
                    max_workers=max_workers,
                )

        logger.info(&#34;Job Done!&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="smdt_rdv.job_manager.worker.smdt_worker"><code class="flex name class">
<span>class <span class="ident">smdt_worker</span></span>
<span>(</span><span>yaml_path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Core class to execute the sMDT RDV job based on given cfg file</p>
<p>Initialize executor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to yaml job config file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class smdt_worker(object):
    &#34;&#34;&#34;Core class to execute the sMDT RDV job based on given cfg file&#34;&#34;&#34;

    def __init__(self, yaml_path: str) -&gt; None:
        &#34;&#34;&#34;Initialize executor.

        Args:
            yaml_path (str): path to yaml job config file
        &#34;&#34;&#34;
        self.job_config = None
        self.load_config(yaml_path)

    def get_config(self):
        return self.job_config

    def load_config(self, yaml_path: str) -&gt; None:
        &#34;&#34;&#34;Retrieves configurations from yaml file

        Args:
            yaml_path (str): path to yaml job config file
        &#34;&#34;&#34;
        cfg_path = job_utils.get_valid_cfg_path(yaml_path)
        yaml_dict = config_utils.load_yaml_dict(cfg_path)
        job_config_temp = config_utils.sMDT_Job_Config(yaml_dict)
        # Check whether need to import other (default) ini file first
        if hasattr(job_config_temp, &#34;config&#34;):
            if hasattr(job_config_temp.config, &#34;include&#34;):
                import_ini_path_list = job_config_temp.config.include
                if import_ini_path_list:
                    for cfg_path in import_ini_path_list:
                        self.load_config(cfg_path)
                        logger.info(f&#34;Included config: {cfg_path}&#34;)
        if self.job_config:
            self.job_config.update(yaml_dict)
        else:
            self.job_config = job_config_temp

        datestr = datetime.date.today().strftime(&#34;%Y-%m-%d&#34;)
        job_cfg = self.job_config.job
        run_cfg = self.job_config.run
        run_cfg.datestr = datestr
        run_cfg.config_collected = True

    def work(self) -&gt; None:
        &#34;&#34;&#34;Execute all planned jobs&#34;&#34;&#34;
        self.job_config.print()
        job_type = self.job_config.job.job_type
        if job_type == &#34;dump_database&#34;:
            self.work_on_database()
        elif job_type == &#34;analyze&#34;:
            self.work_on_analyze()
        else:
            logger.critical(
                f&#34;Unknown job type: {job_type}, please check &#39;job.job_type&#39;&#34;
            )
            exit(1)

    def work_on_database(self) -&gt; None:
        &#34;&#34;&#34;Dump events database according to configs&#34;&#34;&#34;
        full_config = self.job_config
        job_cfg = full_config.job
        path_cfg = full_config.path
        run_cfg = full_config.run
        perf_cfg = full_config.performance

        # initialization
        data_time = str(datetime.datetime.now())
        logger.info(f&#34;Data processed at {data_time}&#34;)

        # check output directory
        if path_cfg.database_out_dir == config_utils.UNDEFINED_CFG_STR:
            logger.error(
                &#34;Undefined output database directory, please check &#39;path.database_out_dir&#39;&#34;
            )
            exit(1)
        db_dir = pathlib.Path(path_cfg.database_out_dir)
        if db_dir.exists():
            logger.info(f&#34;Output database directory: {db_dir} already exists&#34;)
            if path_cfg.overwrite:
                logger.info(f&#34;Remove existing directory as configured&#34;)
                shutil.rmtree(db_dir)
            else:
                logger.critical(
                    f&#34;Please change database_out_dir or set path.overwrite to true&#34;
                )
                exit(1)

        # loop over files
        if path_cfg.data_in_dir == config_utils.UNDEFINED_CFG_STR:
            logger.error(
                &#34;Undefined input data file directory, please check &#39;path.data_in_dir&#39;&#34;
            )
            exit(1)
        data_dir = pathlib.Path(path_cfg.data_in_dir)
        data_list = list(data_dir.rglob(&#34;*.data&#34;))
        data_list += list(data_dir.rglob(&#34;*.data.gz&#34;))
        num_data_files = len(data_list)
        if num_data_files &gt; 0:
            logger.info(f&#34;Number of data file found: {num_data_files}&#34;)
        else:
            logger.warn(&#34;No data file found!&#34;)
        run_cfg.out_dir = pathlib.Path(path_cfg.database_out_dir)
        run_cfg.out_dir.mkdir(exist_ok=True, parents=True)
        max_files = job_cfg.max_files
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=perf_cfg.max_workers
        ) as process_executor:
            for file_id, data_path in enumerate(data_list):
                # prepare config, because only object can be pickled can work with multi-processing
                config_dict = copy.deepcopy(full_config.get_config_dict())
                # dump database
                if max_files &gt; 0 and file_id &gt;= max_files:
                    break
                logger.info(f&#34;Arranging to process data file: {data_path}&#34;)
                if perf_cfg.multi_process:
                    process_executor.submit(
                        db_dumper.dump_database_single_file, data_path, config_dict
                    )
                else:
                    db_dumper.dump_database_single_file(data_path, config_dict)

        # TODO: need to check output (shouldn&#39;t end with .tmp.db if success)
        #       if fail, try to decrease cache_size (for example 1/2) and try again
        #       as has been observed, too large cache_size may cause job to fail

    def work_on_analyze(self) -&gt; None:
        &#34;&#34;&#34;Dump events database according to configs&#34;&#34;&#34;
        full_config = self.job_config
        job_cfg = full_config.job
        path_cfg = full_config.path
        run_cfg = full_config.run
        perf_cfg = full_config.performance

        # initialization
        data_time = str(datetime.datetime.now())
        logger.info(f&#34;Data processed at {data_time}&#34;)

        ## check output directory
        result_dir = pathlib.Path(path_cfg.results_out_dir)
        if result_dir.exists():
            logger.info(f&#34;Result directory: {result_dir} already exists&#34;)
            if path_cfg.overwrite:
                logger.info(f&#34;Remove existing directory as configured&#34;)
                shutil.rmtree(result_dir)
            else:
                logger.critical(
                    f&#34;Please change results_out_dir or set path.overwrite to true&#34;
                )
                exit(1)

        ## check input database directory
        input_db_dirs = []
        if path_cfg.database_in_dir == config_utils.UNDEFINED_CFG_STR:
            logger.error(
                &#34;Undefined input database directory, please check &#39;path.database_in_dir&#39;&#34;
            )
            exit(1)
        db_in = path_cfg.database_in_dir
        if type(db_in) is list:
            for db_ele in db_in:
                input_db_dirs.append(pathlib.Path(db_ele))
        elif type(db_in) is str:
            input_db_dirs.append(pathlib.Path(db_in))
        else:
            logger.critical(
                f&#34;Unsupported database_in_dir type {type(db_in)}, please check &#39;path.data_in_dir&#39;&#34;
            )
            exit(1)
        data_list = []
        for db_dir in input_db_dirs:
            data_list += list(db_dir.rglob(&#34;*data.db&#34;))

        num_data_files = len(data_list)
        if num_data_files &gt; 0:
            logger.info(
                f&#34;Number of data file found: {num_data_files} in {len(input_db_dirs)} database directories&#34;
            )
        else:
            logger.warn(&#34;No data file found!&#34;)

        # loop over files
        max_files = job_cfg.max_files
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=perf_cfg.max_workers
        ) as process_executor:
            # prepare
            db_paths = []
            file_ids = []
            config_dict = copy.deepcopy(full_config.get_config_dict())
            # loop files
            results = []
            for file_id, db_path in enumerate(data_list):
                if max_files &gt; 0 and file_id &gt;= max_files:
                    break
                if perf_cfg.multi_process:
                    # add to multi-process input if multi-process enabled
                    db_paths.append(db_path)
                    file_ids.append(file_id)
                else:
                    # otherwise run on current input directly
                    single_result = analyzer.analyze_single_file(
                        db_path, file_id, config_dict
                    )
                    results.append(single_result)
            # run multi-process
            if perf_cfg.multi_process:
                logger.info(&#34;Analyzing with multi-processing enabled&#34;)

                # get chunk size and input chunks
                chunk_size = perf_cfg.ana_chunk_size
                if chunk_size is None or chunk_size &lt;= 0:
                    chunk_size = os.cpu_count()
                db_paths_chunks = [
                    db_paths[x : x + chunk_size]
                    for x in range(0, len(db_paths), chunk_size)
                ]
                file_ids_chunks = [
                    file_ids[x : x + chunk_size]
                    for x in range(0, len(file_ids), chunk_size)
                ]

                # get max results size
                max_results_size = perf_cfg.ana_max_results_cache
                if max_results_size is None or max_results_size &lt;= 0:
                    max_results_size = chunk_size

                # multi-process run
                for db_paths_chunk, file_ids_chunk in zip(
                    db_paths_chunks, file_ids_chunks
                ):
                    temp_results = process_executor.map(
                        analyzer.analyze_single_file,
                        db_paths_chunk,
                        file_ids_chunk,
                        repeat(config_dict),
                    )
                    results += temp_results
                    # reduce results size to save memory
                    if len(results) &gt;= max_results_size:
                        merged_result = {}
                        smdt_spectrums_lists = {
                            chamber_name: [] for chamber_name in job_cfg.chamber_names
                        }
                        for result in results:
                            for chamber_name in job_cfg.chamber_names:
                                smdt_spectrums_lists[chamber_name].append(
                                    result[chamber_name]
                                )

                        # process data
                        for chamber_name in job_cfg.chamber_names:
                            chamber_spectrums = spectrums.merge_spectrums_list(
                                smdt_spectrums_lists[chamber_name]
                            )
                            merged_result[chamber_name] = chamber_spectrums
                        results = [merged_result]

        # collect data from multiprocess outputs
        smdt_spectrums_lists = {
            chamber_name: [] for chamber_name in job_cfg.chamber_names
        }
        for result in results:
            for chamber_name in job_cfg.chamber_names:
                smdt_spectrums_lists[chamber_name].append(result[chamber_name])

        # process data
        for chamber_name in job_cfg.chamber_names:
            # merge spectrums
            logger.info(f&#34;Processing chamber {chamber_name}&#34;)
            logger.info(&#34;Merging spectrums...&#34;)
            chamber_spectrums = spectrums.merge_spectrums_list(
                smdt_spectrums_lists[chamber_name]
            )

            # check hits
            for mezz in range(MAXNUMBERMEZZANINE):
                for channel in range(MAXNUMBERCHANNEL):
                    num_hits = np.sum(chamber_spectrums[mezz][channel][&#34;tdc_lead&#34;])
                    logger.debug(f&#34;mezz {mezz} ch {channel} hits:{num_hits}&#34;)

            # plot
            range_tdc = None
            plot_range_tdc = job_cfg.spectrums_cfg.plot_range_tdc
            if plot_range_tdc:
                if (plot_range_tdc.min is not None) and (
                    plot_range_tdc.max is not None
                ):
                    range_tdc = (plot_range_tdc.min, plot_range_tdc.max)
            range_adc = None
            plot_range_adc = job_cfg.spectrums_cfg.plot_range_adc
            if plot_range_adc:
                if (plot_range_adc.min is not None) and (
                    plot_range_adc.max is not None
                ):
                    range_adc = (plot_range_adc.min, plot_range_adc.max)
            if perf_cfg.multi_process:
                max_workers = perf_cfg.max_workers
            else:
                max_workers = 1
            if job_cfg.book_spectrums:
                spectrums.plot_spectrums(
                    chamber_spectrums,
                    chamber_name=chamber_name,
                    save_dir=pathlib.Path(path_cfg.results_out_dir).joinpath(
                        f&#34;{chamber_name}/spectrums&#34;
                    ),
                    unit=job_cfg.spectrums_cfg.unit,
                    plot_bins_tdc=job_cfg.spectrums_cfg.plot_bins_tdc,
                    plot_range_tdc=range_tdc,
                    plot_bins_adc=job_cfg.spectrums_cfg.plot_bins_adc,
                    plot_range_adc=range_adc,
                    max_workers=max_workers,
                )

        logger.info(&#34;Job Done!&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="smdt_rdv.job_manager.worker.smdt_worker.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    return self.job_config</code></pre>
</details>
</dd>
<dt id="smdt_rdv.job_manager.worker.smdt_worker.load_config"><code class="name flex">
<span>def <span class="ident">load_config</span></span>(<span>self, yaml_path: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves configurations from yaml file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to yaml job config file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_config(self, yaml_path: str) -&gt; None:
    &#34;&#34;&#34;Retrieves configurations from yaml file

    Args:
        yaml_path (str): path to yaml job config file
    &#34;&#34;&#34;
    cfg_path = job_utils.get_valid_cfg_path(yaml_path)
    yaml_dict = config_utils.load_yaml_dict(cfg_path)
    job_config_temp = config_utils.sMDT_Job_Config(yaml_dict)
    # Check whether need to import other (default) ini file first
    if hasattr(job_config_temp, &#34;config&#34;):
        if hasattr(job_config_temp.config, &#34;include&#34;):
            import_ini_path_list = job_config_temp.config.include
            if import_ini_path_list:
                for cfg_path in import_ini_path_list:
                    self.load_config(cfg_path)
                    logger.info(f&#34;Included config: {cfg_path}&#34;)
    if self.job_config:
        self.job_config.update(yaml_dict)
    else:
        self.job_config = job_config_temp

    datestr = datetime.date.today().strftime(&#34;%Y-%m-%d&#34;)
    job_cfg = self.job_config.job
    run_cfg = self.job_config.run
    run_cfg.datestr = datestr
    run_cfg.config_collected = True</code></pre>
</details>
</dd>
<dt id="smdt_rdv.job_manager.worker.smdt_worker.work"><code class="name flex">
<span>def <span class="ident">work</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Execute all planned jobs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def work(self) -&gt; None:
    &#34;&#34;&#34;Execute all planned jobs&#34;&#34;&#34;
    self.job_config.print()
    job_type = self.job_config.job.job_type
    if job_type == &#34;dump_database&#34;:
        self.work_on_database()
    elif job_type == &#34;analyze&#34;:
        self.work_on_analyze()
    else:
        logger.critical(
            f&#34;Unknown job type: {job_type}, please check &#39;job.job_type&#39;&#34;
        )
        exit(1)</code></pre>
</details>
</dd>
<dt id="smdt_rdv.job_manager.worker.smdt_worker.work_on_analyze"><code class="name flex">
<span>def <span class="ident">work_on_analyze</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Dump events database according to configs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def work_on_analyze(self) -&gt; None:
    &#34;&#34;&#34;Dump events database according to configs&#34;&#34;&#34;
    full_config = self.job_config
    job_cfg = full_config.job
    path_cfg = full_config.path
    run_cfg = full_config.run
    perf_cfg = full_config.performance

    # initialization
    data_time = str(datetime.datetime.now())
    logger.info(f&#34;Data processed at {data_time}&#34;)

    ## check output directory
    result_dir = pathlib.Path(path_cfg.results_out_dir)
    if result_dir.exists():
        logger.info(f&#34;Result directory: {result_dir} already exists&#34;)
        if path_cfg.overwrite:
            logger.info(f&#34;Remove existing directory as configured&#34;)
            shutil.rmtree(result_dir)
        else:
            logger.critical(
                f&#34;Please change results_out_dir or set path.overwrite to true&#34;
            )
            exit(1)

    ## check input database directory
    input_db_dirs = []
    if path_cfg.database_in_dir == config_utils.UNDEFINED_CFG_STR:
        logger.error(
            &#34;Undefined input database directory, please check &#39;path.database_in_dir&#39;&#34;
        )
        exit(1)
    db_in = path_cfg.database_in_dir
    if type(db_in) is list:
        for db_ele in db_in:
            input_db_dirs.append(pathlib.Path(db_ele))
    elif type(db_in) is str:
        input_db_dirs.append(pathlib.Path(db_in))
    else:
        logger.critical(
            f&#34;Unsupported database_in_dir type {type(db_in)}, please check &#39;path.data_in_dir&#39;&#34;
        )
        exit(1)
    data_list = []
    for db_dir in input_db_dirs:
        data_list += list(db_dir.rglob(&#34;*data.db&#34;))

    num_data_files = len(data_list)
    if num_data_files &gt; 0:
        logger.info(
            f&#34;Number of data file found: {num_data_files} in {len(input_db_dirs)} database directories&#34;
        )
    else:
        logger.warn(&#34;No data file found!&#34;)

    # loop over files
    max_files = job_cfg.max_files
    with concurrent.futures.ProcessPoolExecutor(
        max_workers=perf_cfg.max_workers
    ) as process_executor:
        # prepare
        db_paths = []
        file_ids = []
        config_dict = copy.deepcopy(full_config.get_config_dict())
        # loop files
        results = []
        for file_id, db_path in enumerate(data_list):
            if max_files &gt; 0 and file_id &gt;= max_files:
                break
            if perf_cfg.multi_process:
                # add to multi-process input if multi-process enabled
                db_paths.append(db_path)
                file_ids.append(file_id)
            else:
                # otherwise run on current input directly
                single_result = analyzer.analyze_single_file(
                    db_path, file_id, config_dict
                )
                results.append(single_result)
        # run multi-process
        if perf_cfg.multi_process:
            logger.info(&#34;Analyzing with multi-processing enabled&#34;)

            # get chunk size and input chunks
            chunk_size = perf_cfg.ana_chunk_size
            if chunk_size is None or chunk_size &lt;= 0:
                chunk_size = os.cpu_count()
            db_paths_chunks = [
                db_paths[x : x + chunk_size]
                for x in range(0, len(db_paths), chunk_size)
            ]
            file_ids_chunks = [
                file_ids[x : x + chunk_size]
                for x in range(0, len(file_ids), chunk_size)
            ]

            # get max results size
            max_results_size = perf_cfg.ana_max_results_cache
            if max_results_size is None or max_results_size &lt;= 0:
                max_results_size = chunk_size

            # multi-process run
            for db_paths_chunk, file_ids_chunk in zip(
                db_paths_chunks, file_ids_chunks
            ):
                temp_results = process_executor.map(
                    analyzer.analyze_single_file,
                    db_paths_chunk,
                    file_ids_chunk,
                    repeat(config_dict),
                )
                results += temp_results
                # reduce results size to save memory
                if len(results) &gt;= max_results_size:
                    merged_result = {}
                    smdt_spectrums_lists = {
                        chamber_name: [] for chamber_name in job_cfg.chamber_names
                    }
                    for result in results:
                        for chamber_name in job_cfg.chamber_names:
                            smdt_spectrums_lists[chamber_name].append(
                                result[chamber_name]
                            )

                    # process data
                    for chamber_name in job_cfg.chamber_names:
                        chamber_spectrums = spectrums.merge_spectrums_list(
                            smdt_spectrums_lists[chamber_name]
                        )
                        merged_result[chamber_name] = chamber_spectrums
                    results = [merged_result]

    # collect data from multiprocess outputs
    smdt_spectrums_lists = {
        chamber_name: [] for chamber_name in job_cfg.chamber_names
    }
    for result in results:
        for chamber_name in job_cfg.chamber_names:
            smdt_spectrums_lists[chamber_name].append(result[chamber_name])

    # process data
    for chamber_name in job_cfg.chamber_names:
        # merge spectrums
        logger.info(f&#34;Processing chamber {chamber_name}&#34;)
        logger.info(&#34;Merging spectrums...&#34;)
        chamber_spectrums = spectrums.merge_spectrums_list(
            smdt_spectrums_lists[chamber_name]
        )

        # check hits
        for mezz in range(MAXNUMBERMEZZANINE):
            for channel in range(MAXNUMBERCHANNEL):
                num_hits = np.sum(chamber_spectrums[mezz][channel][&#34;tdc_lead&#34;])
                logger.debug(f&#34;mezz {mezz} ch {channel} hits:{num_hits}&#34;)

        # plot
        range_tdc = None
        plot_range_tdc = job_cfg.spectrums_cfg.plot_range_tdc
        if plot_range_tdc:
            if (plot_range_tdc.min is not None) and (
                plot_range_tdc.max is not None
            ):
                range_tdc = (plot_range_tdc.min, plot_range_tdc.max)
        range_adc = None
        plot_range_adc = job_cfg.spectrums_cfg.plot_range_adc
        if plot_range_adc:
            if (plot_range_adc.min is not None) and (
                plot_range_adc.max is not None
            ):
                range_adc = (plot_range_adc.min, plot_range_adc.max)
        if perf_cfg.multi_process:
            max_workers = perf_cfg.max_workers
        else:
            max_workers = 1
        if job_cfg.book_spectrums:
            spectrums.plot_spectrums(
                chamber_spectrums,
                chamber_name=chamber_name,
                save_dir=pathlib.Path(path_cfg.results_out_dir).joinpath(
                    f&#34;{chamber_name}/spectrums&#34;
                ),
                unit=job_cfg.spectrums_cfg.unit,
                plot_bins_tdc=job_cfg.spectrums_cfg.plot_bins_tdc,
                plot_range_tdc=range_tdc,
                plot_bins_adc=job_cfg.spectrums_cfg.plot_bins_adc,
                plot_range_adc=range_adc,
                max_workers=max_workers,
            )

    logger.info(&#34;Job Done!&#34;)</code></pre>
</details>
</dd>
<dt id="smdt_rdv.job_manager.worker.smdt_worker.work_on_database"><code class="name flex">
<span>def <span class="ident">work_on_database</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Dump events database according to configs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def work_on_database(self) -&gt; None:
    &#34;&#34;&#34;Dump events database according to configs&#34;&#34;&#34;
    full_config = self.job_config
    job_cfg = full_config.job
    path_cfg = full_config.path
    run_cfg = full_config.run
    perf_cfg = full_config.performance

    # initialization
    data_time = str(datetime.datetime.now())
    logger.info(f&#34;Data processed at {data_time}&#34;)

    # check output directory
    if path_cfg.database_out_dir == config_utils.UNDEFINED_CFG_STR:
        logger.error(
            &#34;Undefined output database directory, please check &#39;path.database_out_dir&#39;&#34;
        )
        exit(1)
    db_dir = pathlib.Path(path_cfg.database_out_dir)
    if db_dir.exists():
        logger.info(f&#34;Output database directory: {db_dir} already exists&#34;)
        if path_cfg.overwrite:
            logger.info(f&#34;Remove existing directory as configured&#34;)
            shutil.rmtree(db_dir)
        else:
            logger.critical(
                f&#34;Please change database_out_dir or set path.overwrite to true&#34;
            )
            exit(1)

    # loop over files
    if path_cfg.data_in_dir == config_utils.UNDEFINED_CFG_STR:
        logger.error(
            &#34;Undefined input data file directory, please check &#39;path.data_in_dir&#39;&#34;
        )
        exit(1)
    data_dir = pathlib.Path(path_cfg.data_in_dir)
    data_list = list(data_dir.rglob(&#34;*.data&#34;))
    data_list += list(data_dir.rglob(&#34;*.data.gz&#34;))
    num_data_files = len(data_list)
    if num_data_files &gt; 0:
        logger.info(f&#34;Number of data file found: {num_data_files}&#34;)
    else:
        logger.warn(&#34;No data file found!&#34;)
    run_cfg.out_dir = pathlib.Path(path_cfg.database_out_dir)
    run_cfg.out_dir.mkdir(exist_ok=True, parents=True)
    max_files = job_cfg.max_files
    with concurrent.futures.ProcessPoolExecutor(
        max_workers=perf_cfg.max_workers
    ) as process_executor:
        for file_id, data_path in enumerate(data_list):
            # prepare config, because only object can be pickled can work with multi-processing
            config_dict = copy.deepcopy(full_config.get_config_dict())
            # dump database
            if max_files &gt; 0 and file_id &gt;= max_files:
                break
            logger.info(f&#34;Arranging to process data file: {data_path}&#34;)
            if perf_cfg.multi_process:
                process_executor.submit(
                    db_dumper.dump_database_single_file, data_path, config_dict
                )
            else:
                db_dumper.dump_database_single_file(data_path, config_dict)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="smdt_rdv.job_manager" href="index.html">smdt_rdv.job_manager</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="smdt_rdv.job_manager.worker.smdt_worker" href="#smdt_rdv.job_manager.worker.smdt_worker">smdt_worker</a></code></h4>
<ul class="">
<li><code><a title="smdt_rdv.job_manager.worker.smdt_worker.get_config" href="#smdt_rdv.job_manager.worker.smdt_worker.get_config">get_config</a></code></li>
<li><code><a title="smdt_rdv.job_manager.worker.smdt_worker.load_config" href="#smdt_rdv.job_manager.worker.smdt_worker.load_config">load_config</a></code></li>
<li><code><a title="smdt_rdv.job_manager.worker.smdt_worker.work" href="#smdt_rdv.job_manager.worker.smdt_worker.work">work</a></code></li>
<li><code><a title="smdt_rdv.job_manager.worker.smdt_worker.work_on_analyze" href="#smdt_rdv.job_manager.worker.smdt_worker.work_on_analyze">work_on_analyze</a></code></li>
<li><code><a title="smdt_rdv.job_manager.worker.smdt_worker.work_on_database" href="#smdt_rdv.job_manager.worker.smdt_worker.work_on_database">work_on_database</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>